{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CV Bootcamp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall goals:\n",
    "- Train object detection model\n",
    "- Evaluate model performance\n",
    "- Test model on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/bombus_bbox.jpg\" alt=\"Bee bbox\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## **Process outline:**\n",
    "### Collect data >> QC/organize data >> Annotate images >> Train model >> Evaluate and retrain >> Test and deploy model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We have three image datasets: \n",
    "\n",
    "### Fly images\n",
    "- Collected in field and lab\n",
    "- Aim: find all flies and categorize by species/sex\n",
    "\n",
    "### Cocoon images\n",
    "- Collected over two years; includes 3 species from genus *Osmia*\n",
    "- Aim: classify cocoons by species\n",
    "\n",
    "### Camera trap images\n",
    "- Collected over two years at various field locations\n",
    "- +2k images annotated to class/genus/species\n",
    "- Aim: find all insects and classify to lowest taxon possible\n",
    "# \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **1. Annotation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/labelimg.jpeg\" alt=\"Bee bbox\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An effective model depends on a well annotated dataset.\n",
    "- Try to balance speed with accuracy \n",
    "    - Bounding boxes should be tight around the objects, but we can allow room for error during training so that a predicted box is counted as 'good' if it's close enough.    \n",
    "- It's important that objects aren't missed to avoid punishing the algorithm for making a correct determination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow the README in the ANNOTATION directory to annotate a few practice images. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. DATASET ORGANIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The full annotated image dataset is split at least into a training and validation set; a test set is optional but ideal.\n",
    "- The algorithm learns on the training set, the validation set is used to evaluate its performance after each iteration. \n",
    "- Common train/val splits are 70/30 or 80/20\n",
    "\n",
    "    - The **training set** should be a large, diverse set representing the range of images you expect to encounter\n",
    "    - The **validation set** should be representative of the objects you want to detect (don't use 100 images of flies and 2 images of bees if you need to find bees)\n",
    "    \n",
    "- Be careful to avoid including similar or the same images in the train and val sets\n",
    "    - With event-based datasets, such as the fly and camera trap sets, a temporal split is best, e.g. the first 7 days are the training set, the last 3 days are the val set. A random 70/30 split could end up including similar images a few seconds apart in either set.\n",
    "    - A random split works well for images that are entirely independent, such as the cocoon dataset. In this instance you could apply k-fold cross validation, which systematically trains the model on different train/val splits and averages the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random subset of the annotated camera trap dataset is available [here](https://oregonstate.box.com/s/rk2loajko18ny6fsm0nl0rsmagon1p2j) for practice.\n",
    " \n",
    "Or run the following code to download it to this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 2_DATASET/ && curl -L -o 2_DATASET/practice_set.zip \"https://app.box.com/shared/static/rk2loajko18ny6fsm0nl0rsmagon1p2j\" && unzip -o 2_DATASET/practice_set.zip -d 2_DATASET/ && rm 2_DATASET/practice_set.zip 2_DATASET/practice_dataset/.delete.sh.swp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a minute to familiarize yourself with the train and val datasets\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/yolov8.jpeg\" alt=\"Bee bbox\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your dataset annotated and organized, you can start training. For object detection, we're currently using the [YOLOv8](https://yolov8.com/) framework. \n",
    "\n",
    "In the TRAINING directory there is a basic yolo training script. Try it out on the practice dataset. \n",
    "\n",
    "Training can be done on a CPU but works best using a GPU. If local GPU access is limited, you can follow [this YOLOv8 tutorial](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb), which has a lot of extraneous Roboflow information but gets the point across. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
