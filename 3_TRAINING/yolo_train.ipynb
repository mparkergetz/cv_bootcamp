{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a YOLO model\n",
    "___\n",
    "Assuming miniconda3 was installed during the annotation step, let's set up a new environment, yolov8. The environment.yml file in this dir can be used to install the necessary packages. \n",
    "\n",
    "In a terminal navigate to this current directory and run the following:\n",
    "\n",
    "<code>conda env create -f environment.yml</code>\n",
    "\n",
    "Then select the yolov8 kernel for this notebook under 'Python environments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import os\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two config yamls in this dir, practice_dataset.yaml and config.yaml.\n",
    "\n",
    "practice_dataset.yaml defines the data location. Pointing YOLO to your data can be frustrating because there is a settings.yaml file (e.g. in ~/.config/Ultralytics) that specifies a root 'datasets_dir' that may confuse the path in your dataset.yaml. If the dataset is not found when you try to train the model, the error will specify where you need to look for the settings.yaml file. \n",
    "\n",
    "config.yaml is where you set metadata for the training run and hyperparameters. The file has descriptions of each of the settings, but here are some important ones:\n",
    "- **model**: choose which pretrained yolo model to fine-tune (yolov8n = nano, yolov8s = small, etc.). Smaller models will train faster, larger models take longer to train and run, but are suited for very complex detection tasks. \n",
    "- **data**: make sure it points to the right dataset.yaml (practice_dataset.yaml in our case)\n",
    "- **epochs**: One epoch is one train/val pass on the whole dataset. Start with just a few epochs (10) to troubleshoot your dataset/hyperparameter configuration, then train on 100-200 epochs when you have your settings dialed in\n",
    "- **batch**: with a single GPU, Autobatch (-1) is great so you can set different img sizes and it'll adjust to the GPU available. Autobatch isn't available when you're training in parallel on multiple GPUs (afaik). The larger the batch size, the better - the algorith is 'learning' on more information each step. Your batch size will be limited by your GPU VRAM availability. \n",
    "- **imgsz**: imgs can be resized for more efficient training (imgsz: 640 resizes each img to 640x640). Smaller images mean more images per batch, but if the details we want to detect are small (flies, etc.), shrinking the size can obscure necessary details.\n",
    "- **device**: useful if working with multiple GPUs or on a cluster\n",
    "- **project**: name your project to keep it organized, especially if exporting results to WandB\n",
    "- **optimizer**: you may want to specify SGD or one of the Adams based on the application, or experiment between several\n",
    "- **close mosaic**: we'll get into the mosaic augmentation later, worth noting this setting\n",
    "- **iou**: set the intersection over union threshold for Non-max Suppression (NMS). Decrease this if youre getting a lot of boxes duplicated over the same object. \n",
    "\n",
    "### Hyperparameters and Augmentations\n",
    "Augmentations can be randomly applied to images during training to simulate image variation in real conditions (light source, different lenses, time of day, etc.). Augmenting images can make the model more robust to new data, but only use augmentations that make sense, e.g. don't vary the hue wildly if object color is important in classfying the object (fine if just detecting).\n",
    "- **lr0 and lrf**: initial and final learning rates. important to experiment with if not automated\n",
    "- **hsv_h, hsv_s, hsv_v**: vary the hue, saturation, and value (brightness) randomly by the percentage provided\n",
    "- **scale, flipud, fliplr**: scale can be useful to account for changes in object size, flipping works well in the case of the fly or camera trap imgs because they are top-down (it wouldn't make sense to flipud an img of a giraffe).\n",
    "- **mosaic**: a useful augmentation that makes a composite from the corners of 4 random images in the batch. This works well to shuffle the data even more during training and accommodate variation in unseen data.\n",
    "\n",
    "Familiarize yourself with all the settings and consider their applicability to the dataset you're working on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ultralytics.cfg.get_cfg(cfg='config.yaml') # Load in cfg\n",
    "model = YOLO(\"yolov8n.pt\") # Load pretrained model. It will download if it's not directly available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains the model. You'll likely have to adjust the root dataset path in the .config/Ultralytics/settings.yaml file to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.train(cfg='config.yaml') # run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training metrics look pretty good, but go into the output dir (practice_runs/train#). The output dir contains the weights and metrics reports. Look through all the metric reports and get a sense for where the model is struggling.\n",
    "\n",
    "The train_batch#.jpg and val_batch#_pred.jpg imgs are useful to see what the model is being trained on and get a small snapshot of how the model is doing on the val set. By comparing the val_labels and val_pred jpgs, we can see that very few insects in the val set are being detected. The Recall-confidence curve (R_curve) is also very poor, and the confusion_matrix shows that few insects are being detected. \n",
    "\n",
    "What does it mean when the training metrics are high but the validation performance is low?\n",
    "___\n",
    "\n",
    "We can apply our trained model to new images now. If your model isn't already defined (as it is during training above), you can load any model from the output weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('practice_runs/train/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the model to run inference on a new image. This image happens to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# load in img\n",
    "image_path = \"../1_ANNOTATION/jpgs_to_annot/pi1_20240721_091114.jpg\" \n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# run model on image\n",
    "results = model(image)\n",
    "\n",
    "# plot results\n",
    "for r in results:\n",
    "        im_array = r.plot()\n",
    "        im = Image.fromarray(im_array[..., ::-1]) \n",
    "        plt.imshow(im)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
